
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Optimized Reconciliation Tool (Streaming CSV / XLSX)</title>

  <!-- Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/papaparse@5.4.1/papaparse.min.js"></script>
  <script src="https://unpkg.com/xlsx/dist/xlsx.full.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/idb-keyval@6/dist/umd.js"></script>

  <!-- Tailwind CDN for quick styling -->
  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    /* small helpers */
    .small { font-size: .9rem; }
    .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Helvetica Neue", monospace; }
  </style>
</head>
<body class="bg-gray-50 min-h-screen">
  <div class="max-w-6xl mx-auto p-6">
    <div class="bg-white rounded-lg shadow p-6">
      <h1 class="text-2xl font-bold mb-2">Optimized Reconciliation Tool — Streaming-friendly</h1>
      <p class="text-sm text-gray-600 mb-4">This version uses streaming parsing and IndexedDB to handle very large CSVs (recommended) and large XLSX by converting sheet->CSV then streaming. Read stage happens immediately on upload (headers, sample, and counts are stored) so you can configure matching before full reconciliation.</p>

      <!-- Step 1: Upload & read -->
      <section class="mb-6">
        <h2 class="font-semibold">Step 1 — Upload files (reads immediately)</h2>
        <div class="grid md:grid-cols-2 gap-4 mt-3">
          <div class="p-4 border rounded bg-gray-50">
            <label class="block font-medium mb-2">Category A files (CSV / XLSX / XLS)</label>
            <input id="inputA" type="file" multiple accept=".csv,.txt,.xlsx,.xls" class="block" />
            <div id="statusA" class="mt-3 text-sm small text-gray-700"></div>
          </div>

          <div class="p-4 border rounded bg-gray-50">
            <label class="block font-medium mb-2">Category B files (CSV / XLSX / XLS)</label>
            <input id="inputB" type="file" multiple accept=".csv,.txt,.xlsx,.xls" class="block" />
            <div id="statusB" class="mt-3 text-sm small text-gray-700"></div>
          </div>
        </div>

        <div class="mt-4 text-sm text-gray-700">
          <p><strong>Notes:</strong> For extremely large datasets (millions of rows) CSV is strongly recommended — browsers can stream CSV with low memory. XLSX files are converted to CSV internally and then streamed; this still requires some memory but is much better than loading full JSON.</p>
        </div>
      </section>

      <!-- Display extracted headers and sample -->
      <section class="mb-6">
        <h3 class="font-semibold">Detected headers & sample rows (from first file of each category)</h3>
        <div class="grid md:grid-cols-2 gap-4">
          <div class="p-3 border rounded bg-white">
            <div class="flex justify-between items-center">
              <div><strong>Category A</strong></div>
              <div id="metaA" class="text-sm text-gray-600"></div>
            </div>
            <pre id="headersA" class="mono text-xs mt-2 p-2 bg-gray-50 rounded overflow-auto" style="max-height:150px"></pre>
          </div>

          <div class="p-3 border rounded bg-white">
            <div class="flex justify-between items-center">
              <div><strong>Category B</strong></div>
              <div id="metaB" class="text-sm text-gray-600"></div>
            </div>
            <pre id="headersB" class="mono text-xs mt-2 p-2 bg-gray-50 rounded overflow-auto" style="max-height:150px"></pre>
          </div>
        </div>
      </section>

      <!-- Step 2: Configure matching (only after read) -->
      <section class="mb-6">
        <h3 class="font-semibold">Step 2 — Configure Unique ID components & amount columns</h3>
        <div class="grid md:grid-cols-2 gap-4">
          <div class="p-4 border rounded bg-white">
            <div class="mb-2"><strong>Category A</strong></div>
            <label class="block text-xs mb-1">Amount column</label>
            <select id="amountA" class="w-full p-2 border rounded"></select>

            <label class="block text-xs mt-3 mb-1">Unique ID columns (order forms composite key)</label>
            <div id="idColsA" class="space-y-2"></div>
            <button id="addA" class="mt-2 px-3 py-1 bg-indigo-600 text-white rounded">Add component</button>
          </div>

          <div class="p-4 border rounded bg-white">
            <div class="mb-2"><strong>Category B</strong></div>
            <label class="block text-xs mb-1">Amount column</label>
            <select id="amountB" class="w-full p-2 border rounded"></select>

            <label class="block text-xs mt-3 mb-1">Unique ID columns (order forms composite key)</label>
            <div id="idColsB" class="space-y-2"></div>
            <button id="addB" class="mt-2 px-3 py-1 bg-indigo-600 text-white rounded">Add component</button>
          </div>
        </div>

        <div class="mt-4 flex justify-end gap-3">
          <button id="btnReconcile" class="px-4 py-2 bg-green-600 text-white rounded disabled:opacity-50" disabled>Run Reconciliation</button>
        </div>

        <div id="infoBox" class="mt-3 text-sm text-gray-600"></div>
      </section>

      <!-- Results -->
      <section id="results" class="hidden">
        <h3 class="font-semibold">Results & Download</h3>
        <div id="summary" class="mt-3 p-3 bg-white border rounded"></div>
        <div class="mt-3 flex gap-2">
          <button id="downloadMatched" class="px-3 py-2 bg-blue-600 text-white rounded">Download Matched</button>
          <button id="downloadUnmatched" class="px-3 py-2 bg-orange-600 text-white rounded">Download Unmatched</button>
        </div>
      </section>

      <div class="mt-6 text-xs text-gray-500">
        <p><strong>Technical note:</strong> Browsers are not a replacement for server-side ETL when working with multi-million-row datasets. This tool pushes the browser as far as is realistically possible using streaming, IndexedDB, and Web Workers. If you expect >10M rows or need guaranteed stability, I can provide a Node/Server implementation.</p>
      </div>
    </div>
  </div>

<script>
/*
  Overview of approach:
  - CSV files: parsed with PapaParse in worker mode (streaming, chunked).
  - XLSX/XLS: converted to CSV by SheetJS (sheet_to_csv) then streamed via PapaParse (in chunks) to avoid immediate large JSON creation.
  - Minimal row sample and header extraction happens immediately on upload (Step 1).
  - Parsed rows are stored in IndexedDB (idb-keyval) as per-category incremental chunks to avoid keeping everything in memory.
  - Reconciliation reads from IndexedDB in streaming fashion (cursor) and produces results; finally downloads created using XLSX.
  - For very large datasets, ensure you use CSV files whenever possible.
*/

const { get: idbGet, set: idbSet, del: idbDel } = idbKeyval;

const inputA = document.getElementById('inputA');
const inputB = document.getElementById('inputB');
const statusA = document.getElementById('statusA');
const statusB = document.getElementById('statusB');
const headersA = document.getElementById('headersA');
const headersB = document.getElementById('headersB');
const metaA = document.getElementById('metaA');
const metaB = document.getElementById('metaB');
const amountA = document.getElementById('amountA');
const amountB = document.getElementById('amountB');
const idColsA = document.getElementById('idColsA');
const idColsB = document.getElementById('idColsB');
const addA = document.getElementById('addA');
const addB = document.getElementById('addB');
const btnReconcile = document.getElementById('btnReconcile');
const infoBox = document.getElementById('infoBox');
const resultsSection = document.getElementById('results');
const summaryDiv = document.getElementById('summary');
const downloadMatched = document.getElementById('downloadMatched');
const downloadUnmatched = document.getElementById('downloadUnmatched');

let metaStore = {
  A: { files: [], headers: [], sample: [], rowCount: 0, chunks: 0 },
  B: { files: [], headers: [], sample: [], rowCount: 0, chunks: 0 }
};

// Utility: clear previous category data from IndexedDB and memory
async function clearCategory(category) {
  await idbDel('cat_' + category + '_count');
  await idbDel('cat_' + category + '_chunks');
  metaStore[category] = { files: [], headers: [], sample: [], rowCount: 0, chunks: 0 };
}

// Called when user uploads files
inputA.addEventListener('change', e => handleFiles(e.target.files, 'A'));
inputB.addEventListener('change', e => handleFiles(e.target.files, 'B'));

addA.addEventListener('click', () => addIdSelector('A'));
addB.addEventListener('click', () => addIdSelector('B'));

btnReconcile.addEventListener('click', runReconciliation);

// helper to create a new ID selector UI
function addIdSelector(cat) {
  const container = cat === 'A' ? idColsA : idColsB;
  const select = document.createElement('select');
  select.className = 'w-full p-2 border rounded';
  const opt = document.createElement('option');
  opt.value = ''; opt.textContent = 'Select column...';
  select.appendChild(opt);
  (cat === 'A' ? metaStore.A.headers : metaStore.B.headers).forEach(h => {
    const o = document.createElement('option'); o.value = h; o.textContent = h; select.appendChild(o);
  });
  container.appendChild(select);
  // allow removal on double click
  select.title = 'Double-click to remove';
  select.addEventListener('dblclick', () => select.remove());
}

// read & stream files for a category
async function handleFiles(fileList, category) {
  if (!fileList || fileList.length === 0) return;
  // clear previous
  await clearCategory(category);

  metaStore[category].files = Array.from(fileList).map(f => f.name);
  const statusEl = category === 'A' ? statusA : statusB;
  statusEl.textContent = 'Reading files... (this may take time for large files)';

  // reset UI
  (category === 'A' ? headersA : headersB).textContent = '';
  (category === 'A' ? metaA : metaB).textContent = '';

  // keep counts in IndexedDB per category to allow recon later
  let totalRows = 0;
  let chunkIndex = 0;

  for (const file of fileList) {
    statusEl.textContent = `Reading ${file.name}...`;
    const lower = file.name.toLowerCase();
    if (lower.endsWith('.csv') || lower.endsWith('.txt')) {
      // Stream parse CSV with PapaParse (worker enabled)
      await streamCSVFile(file, category, ({ headers, sampleRows, rowsRead, chunksSaved }) => {
        if (metaStore[category].headers.length === 0 && headers) {
          metaStore[category].headers = headers;
          // populate header UI
          (category === 'A' ? headersA : headersB).textContent = headers.join(' | ');
          populateHeaderSelectors();
        }
        // store sample if not present
        if (metaStore[category].sample.length === 0 && sampleRows.length) {
          metaStore[category].sample = sampleRows;
        }
        totalRows += rowsRead;
        chunkIndex += chunksSaved;
      });
    } else if (lower.endsWith('.xlsx') || lower.endsWith('.xls')) {
      // Read XLSX (convert first sheet to csv and stream it)
      await streamXlsxAsCsv(file, category, ({ headers, sampleRows, rowsRead, chunksSaved }) => {
        if (metaStore[category].headers.length === 0 && headers) {
          metaStore[category].headers = headers;
          (category === 'A' ? headersA : headersB).textContent = headers.join(' | ');
          populateHeaderSelectors();
        }
        if (metaStore[category].sample.length === 0 && sampleRows.length) {
          metaStore[category].sample = sampleRows;
        }
        totalRows += rowsRead;
        chunkIndex += chunksSaved;
      });
    } else {
      console.warn('Unsupported file type', file.name);
    }
  } // end files loop

  metaStore[category].rowCount = totalRows;
  metaStore[category].chunks = chunkIndex;
  await idbSet('cat_' + category + '_count', totalRows);
  await idbSet('cat_' + category + '_chunks', chunkIndex);

  statusEl.textContent = `Finished reading. Rows: ${totalRows.toLocaleString()} — chunks: ${chunkIndex}`;
  const metaText = `Rows: ${totalRows.toLocaleString()} | Chunks: ${chunkIndex}`;
  (category === 'A' ? metaA : metaB).textContent = metaText;

  // populate amount selects if headers available
  populateHeaderSelectors();
  infoBox.textContent = 'Files read and stored (in browser IndexedDB). Now configure amount & unique ID components before reconciliation.';
  btnReconcile.disabled = !(metaStore.A.headers.length && metaStore.B.headers.length);
}

// Streaming CSV using PapaParse in worker mode and saving chunked rows into IndexedDB
async function streamCSVFile(file, category, progressCallback) {
  return new Promise((resolve, reject) => {
    const CHUNK_SIZE = 100000; // rows per saved chunk - tune as needed
    let rowsRead = 0;
    let sampleRows = [];
    let headers = null;
    let chunk = [];
    let chunkIndex = 0;

    Papa.parse(file, {
      header: true,
      worker: true,
      skipEmptyLines: true,
      chunkSize: 1024 * 1024, // bytes
      step: async function(row, parser) {
        if (!headers) {
          headers = Object.keys(row.data);
        }
        rowsRead++;
        if (sampleRows.length < 5) sampleRows.push(row.data);
        chunk.push(row.data);

        if (chunk.length >= CHUNK_SIZE) {
          parser.pause();
          // save chunk into IndexedDB under key cat_{category}_chunk_{n}
          const key = `cat_${category}_chunk_${chunkIndex}`;
          await idbSet(key, chunk);
          chunkIndex++;
          chunk = [];
          await idbSet('cat_' + category + '_chunks_temp', chunkIndex);
          parser.resume();
        }
      },
      complete: async function() {
        // save last chunk if any
        if (chunk.length > 0) {
          const key = `cat_${category}_chunk_${chunkIndex}`;
          await idbSet(key, chunk);
          chunkIndex++;
        }
        // store header info
        if (headers) {
          await idbSet('cat_' + category + '_headers', headers);
        }
        // store sample and counts
        await idbSet('cat_' + category + '_sample', sampleRows);
        await idbSet('cat_' + category + '_count_temp', rowsRead);
        resolve(progressCallback({ headers, sampleRows, rowsRead, chunksSaved: chunkIndex }));
      },
      error: function(err) {
        console.error('PapaParse error', err);
        reject(err);
      }
    });
  });
}

// Convert XLSX first sheet to CSV in streaming-friendly manner: we produce CSV text in row-by-row fashion by iterating sheet rows
async function streamXlsxAsCsv(file, category, progressCallback) {
  // Read file arrayBuffer
  const data = await file.arrayBuffer();
  // Using SheetJS: read workbook but do not convert whole sheet to JSON (we'll iterate)
  const wb = XLSX.read(data, { type: 'array' });
  const sheetName = wb.SheetNames[0];
  const ws = wb.Sheets[sheetName];
  // We'll convert in row-major iteration using range
  const range = XLSX.utils.decode_range(ws['!ref'] || 'A1:A1');
  const headers = [];
  let sampleRows = [];
  const CHUNK_SIZE = 100000;
  let rowsRead = 0;
  let chunk = [];
  let chunkIndex = 0;

  // extract headers from first row (r = range.s.r)
  const headerRow = range.s.r;
  for (let c = range.s.c; c <= range.e.c; c++) {
    const cellAddress = { c: c, r: headerRow };
    const cellRef = XLSX.utils.encode_cell(cellAddress);
    const cell = ws[cellRef];
    headers.push(cell ? String(cell.v).trim() : `Column_${c}`);
  }

  // iterate rows starting after header
  for (let r = range.s.r + 1; r <= range.e.r; r++) {
    const rowObj = {};
    for (let c = range.s.c; c <= range.e.c; c++) {
      const cellRef = XLSX.utils.encode_cell({ c: c, r: r });
      const cell = ws[cellRef];
      rowObj[headers[c - range.s.c]] = cell ? cell.v : '';
    }
    rowsRead++;
    if (sampleRows.length < 5) sampleRows.push(rowObj);
    chunk.push(rowObj);

    if (chunk.length >= CHUNK_SIZE) {
      const key = `cat_${category}_chunk_${chunkIndex}`;
      await idbSet(key, chunk);
      chunkIndex++;
      chunk = [];
    }

    // yield occasionally to keep UI responsive
    if (rowsRead % 10000 === 0) {
      await new Promise(res => setTimeout(res, 0));
    }
  }

  if (chunk.length > 0) {
    const key = `cat_${category}_chunk_${chunkIndex}`;
    await idbSet(key, chunk);
    chunkIndex++;
  }

  await idbSet('cat_' + category + '_headers', headers);
  await idbSet('cat_' + category + '_sample', sampleRows);
  await idbSet('cat_' + category + '_count', rowsRead);

  return progressCallback({ headers, sampleRows, rowsRead, chunksSaved: chunkIndex });
}

// populate header selectors
async function populateHeaderSelectors() {
  const hA = (await idbGet('cat_A_headers')) || metaStore.A.headers || [];
  const hB = (await idbGet('cat_B_headers')) || metaStore.B.headers || [];

  // fill amount selects
  amountA.innerHTML = '<option value="">Select...</option>';
  amountB.innerHTML = '<option value="">Select...</option>';
  hA.forEach(h => {
    const o = document.createElement('option'); o.value = h; o.textContent = h; amountA.appendChild(o);
  });
  hB.forEach(h => {
    const o = document.createElement('option'); o.value = h; o.textContent = h; amountB.appendChild(o);
  });

  // show sample for user
  const sa = (await idbGet('cat_A_sample')) || metaStore.A.sample || [];
  const sb = (await idbGet('cat_B_sample')) || metaStore.B.sample || [];
  headersA.textContent = (hA.length ? hA.join(' | ') : 'No headers yet') + '\\n\\nSample:\\n' + JSON.stringify(sa, null, 2);
  headersB.textContent = (hB.length ? hB.join(' | ') : 'No headers yet') + '\\n\\nSample:\\n' + JSON.stringify(sb, null, 2);

  // prepopulate one ID selector each side if none
  if (idColsA.children.length === 0) addIdSelector('A');
  if (idColsB.children.length === 0) addIdSelector('B');

  // enable reconcile if both have headers
  btnReconcile.disabled = !(hA.length && hB.length);
}

// Helper: iterate stored chunks for a category and call handler for each row (async)
async function iterateCategoryRows(category, rowHandler) {
  const chunks = [];
  // gather chunk keys by scanning IndexedDB known keys pattern (we used cat_{category}_chunk_{i})
  // idbKeyval does not provide a keys() method in this lightweight wrapper, so we track count stored earlier
  let chunksCount = await idbGet('cat_' + category + '_chunks');
  if (typeof chunksCount !== 'number') {
    // fallback: try to infer from incremental temp key or scan a reasonable range
    chunksCount = (await idbGet('cat_' + category + '_chunks_temp')) || 0;
  }
  for (let i = 0; i < chunksCount; i++) {
    const key = `cat_${category}_chunk_${i}`;
    const data = await idbGet(key);
    if (Array.isArray(data)) {
      for (const row of data) {
        const stop = await rowHandler(row);
        if (stop) return;
      }
    }
  }
}

// generate composite id from row and list of selected columns (simple concat with underscore)
function generateCompositeFromRow(row, cols) {
  return cols.map(c => {
    const v = row[c];
    if (v === undefined || v === null) return '';
    return String(v).trim();
  }).join('_').toUpperCase();
}

// Reconciliation step: read both categories from IndexedDB streaming and match
async function runReconciliation() {
  infoBox.textContent = 'Running reconciliation — this happens in the browser and may take time depending on dataset size.';
  btnReconcile.disabled = true;
  resultsSection.classList.add('hidden');
  summaryDiv.innerHTML = '';

  // read selected amount & id columns
  const amtA = amountA.value;
  const amtB = amountB.value;
  if (!amtA || !amtB) {
    alert('Please select amount columns for both categories.');
    btnReconcile.disabled = false;
    return;
  }

  // gather id column selections
  const idColsSelectedA = Array.from(idColsA.querySelectorAll('select')).map(s => s.value).filter(Boolean);
  const idColsSelectedB = Array.from(idColsB.querySelectorAll('select')).map(s => s.value).filter(Boolean);
  if (idColsSelectedA.length === 0 || idColsSelectedB.length === 0) {
    alert('Please add at least one Unique ID component on both sides.');
    btnReconcile.disabled = false;
    return;
  }

  // We'll implement a map for Category B keys -> first occurrence row info
  // For memory reasons, if Category B expected rows > 2 million, this will be heavy. We attempt but warn user.
  const countA = (await idbGet('cat_A_count')) || metaStore.A.rowCount || 0;
  const countB = (await idbGet('cat_B_count')) || metaStore.B.rowCount || 0;
  if (countA + countB > 5_000_000) {
    const ok = confirm('Total rows exceed 5M — browser may run out of memory. Do you want to continue? (Consider server-side processing for >5M rows)');
    if (!ok) {
      btnReconcile.disabled = false;
      return;
    }
  }

  infoBox.textContent = 'Loading Category B index into memory (streaming) — this helps fast matching. This may use significant memory for large datasets.';
  // Build map for B
  const mapB = new Map();
  let bRows = 0;
  await iterateCategoryRows('B', async (row) => {
    bRows++;
    const key = generateCompositeFromRow(row, idColsSelectedB);
    const amt = parseFloat(row[amtB]) || 0;
    // store first occurrence (if duplicates exist we will use serial later)
    if (!mapB.has(key)) {
      mapB.set(key, []);
    }
    mapB.get(key).push({ row, amt });
    // keep iterating
    return false;
  });

  infoBox.textContent = `Category B indexed (${bRows.toLocaleString()} rows). Now streaming Category A to match.`;

  // iterate A and try to match with mapB entries
  const matched = [];
  const unmatchedA = [];
  const unmatchedBKeys = new Set(mapB.keys()); // track unused keys to list unmatched B later

  let aRows = 0;
  await iterateCategoryRows('A', async (row) => {
    aRows++;
    const key = generateCompositeFromRow(row, idColsSelectedA);
    const amt = parseFloat(row[amtA]) || 0;
    const candidates = mapB.get(key);
    if (candidates && candidates.length > 0) {
      // simplistic match: match first candidate and remove it
      const match = candidates.shift();
      if (candidates.length === 0) mapB.delete(key);
      matched.push({
        key, 
        aRow: row, 
        bRow: match.row,
        aAmount: amt,
        bAmount: match.amt
      });
      unmatchedBKeys.delete(key);
    } else {
      unmatchedA.push({ key, row, amt });
    }
    // keep UI responsive
    if (aRows % 10000 === 0) {
      summaryDiv.textContent = `Processed ${aRows.toLocaleString()} rows from A... matched: ${matched.length.toLocaleString()}`;
      await new Promise(res => setTimeout(res, 0));
    }
    return false;
  });

  // build unmatchedB list from remaining mapB entries
  const unmatchedB = [];
  for (const [k, arr] of mapB.entries()) {
    for (const item of arr) {
      unmatchedB.push({ key: k, row: item.row, amt: item.amt });
    }
  }

  // show quick summary
  summaryDiv.innerHTML = `
    <div><strong>Summary</strong></div>
    <div>Category A rows processed: ${aRows.toLocaleString()}</div>
    <div>Category B rows indexed: ${bRows.toLocaleString()}</div>
    <div>Matched pairs: ${matched.length.toLocaleString()}</div>
    <div>Unmatched A: ${unmatchedA.length.toLocaleString()}</div>
    <div>Unmatched B: ${unmatchedB.length.toLocaleString()}</div>
  `;
  resultsSection.classList.remove('hidden');

  // store results in idb for download retrieval
  await idbSet('last_matched', matched);
  await idbSet('last_unmatchedA', unmatchedA);
  await idbSet('last_unmatchedB', unmatchedB);

  btnReconcile.disabled = false;
  infoBox.textContent = 'Reconciliation complete (in-browser). Use download buttons to export results.';
}

// Download handlers: create XLSX file from stored results
downloadMatched.addEventListener('click', async () => {
  const matched = (await idbGet('last_matched')) || [];
  if (!matched.length) return alert('No matched results available.');
  // convert to sheet-friendly objects
  const rows = matched.map(m => ({
    Composite_ID: m.key,
    A_Source: m.aRow.Source_File || '',
    B_Source: m.bRow.Source_File || '',
    A_Amount: m.aAmount,
    B_Amount: m.bAmount,
    ...m.aRow,
    ...m.bRow
  }));
  const wb = XLSX.utils.book_new();
  const ws = XLSX.utils.json_to_sheet(rows);
  XLSX.utils.book_append_sheet(wb, ws, 'Matched');
  XLSX.writeFile(wb, 'matched_results.xlsx');
});

downloadUnmatched.addEventListener('click', async () => {
  const unmatchedA = (await idbGet('last_unmatchedA')) || [];
  const unmatchedB = (await idbGet('last_unmatchedB')) || [];
  if (!unmatchedA.length && !unmatchedB.length) return alert('No unmatched results available.');
  const wb = XLSX.utils.book_new();
  if (unmatchedA.length) {
    const rowsA = unmatchedA.map(u => ({ Composite_ID: u.key, Amount: u.amt, ...u.row }));
    XLSX.utils.book_append_sheet(wb, XLSX.utils.json_to_sheet(rowsA), 'Unmatched_A');
  }
  if (unmatchedB.length) {
    const rowsB = unmatchedB.map(u => ({ Composite_ID: u.key, Amount: u.amt, ...u.row }));
    XLSX.utils.book_append_sheet(wb, XLSX.utils.json_to_sheet(rowsB), 'Unmatched_B');
  }
  XLSX.writeFile(wb, 'unmatched_results.xlsx');
});

</script>
</body>
</html>
